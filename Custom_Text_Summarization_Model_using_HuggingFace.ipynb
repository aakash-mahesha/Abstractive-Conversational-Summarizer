{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de10e630-7941-4ffb-b2a7-0f36beead1c9",
   "metadata": {},
   "source": [
    "# Training a custom Text Summarization Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c55082-3ef9-4ec0-8146-03653138f154",
   "metadata": {},
   "source": [
    "- Dependencies to be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25895170-25c7-40ba-adfc-ac0ce62e98b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting GPUtil\n",
      "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
      "Building wheels for collected packages: GPUtil\n",
      "  Building wheel for GPUtil (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=c754e2f657c825bfa69f8b5fb83812d6c305e302005bfea16b707c4d500e7af6\n",
      "  Stored in directory: /home/aakash/.cache/pip/wheels/2b/b5/24/fbb56595c286984f7315ee31821d6121e1b9828436021a88b3\n",
      "Successfully built GPUtil\n",
      "Installing collected packages: GPUtil\n",
      "Successfully installed GPUtil-1.4.0\n",
      "Collecting git-lfs\n",
      "  Downloading git_lfs-1.6-py2.py3-none-any.whl (5.6 kB)\n",
      "Installing collected packages: git-lfs\n",
      "Successfully installed git-lfs-1.6\n"
     ]
    }
   ],
   "source": [
    "#!pip install GPUtil\n",
    "# !pip install transformers\n",
    "#! pip install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631c42b4-3c18-47f9-8ac8-f83ff1a5945f",
   "metadata": {},
   "source": [
    "- GPUtil is a Python module for getting the GPU status from NVIDA GPUs using nvidia-smi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf06035f-a7c1-4a47-b8b7-bcf3eb94c664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  5% | 10% |\n"
     ]
    }
   ],
   "source": [
    "from GPUtil import showUtilization as gpu_usage\n",
    "gpu_usage()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "571abeb7-6995-43e6-95a1-cbdfd3206c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7edcd89c-3953-49d2-a0e5-5bdf643f1dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.3\n",
      "NVIDIA GeForce GTX 1650 Ti\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "143e1c98-21e9-495a-a27d-ad4afdf92be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "491a17de-2410-4521-abe2-0ea94a7a0070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708ebf09-9d0e-4399-884f-70cd5fca15a9",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "- We'll be using the SAMSum dataset (developed by Samsung), which consists of a collection of dialogues along with brief summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6ef9ce9-81fd-44bc-9a14-2dd997fc2594",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset samsum (/home/aakash/.cache/huggingface/datasets/samsum/samsum/0.0.0/3f7dba43be72ab10ca66a2e0f8547b3590e96c2bd9f2cbb1f6bb1ec1f1488ba6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537c2621f5514280981fb0fcffaebefa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "samsum = load_dataset('samsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6af3a68-b4f8-4b0c-ac88-31a481ea99af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33613000-ad54-4392-8699-784746420785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features : ['id', 'dialogue', 'summary']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Features : {samsum['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ed6aa9-1d08-41cd-88ff-3286f53b3582",
   "metadata": {},
   "source": [
    "- Let's us check the dialogue and its summary given in the SAMSUM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9357785b-1fa7-4c5d-98b8-74b4ac166c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue: \n",
      "\n",
      "Amanda: I baked  cookies. Do you want some?\n",
      "Jerry: Sure!\n",
      "Amanda: I'll bring you tomorrow :-)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dialogue: \\n\")\n",
    "print(samsum['train'][0]['dialogue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aa78fac-8a7f-4ccf-a6b7-58db50d3553e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: \n",
      "\n",
      "Amanda baked cookies and will bring Jerry some tomorrow.\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary: \\n\")\n",
    "print(samsum['train'][0]['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8295cf8c-baca-42b0-a4dd-8e941ee145ea",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abb6515-76d2-4583-b907-1d4c1b016bba",
   "metadata": {
    "tags": []
   },
   "source": [
    "# T5 (Text to Text Transfer Transformer) Model\n",
    "- T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format.\n",
    "- T5 works well on a variety of tasks out-of-the-box by prepending a different prefix to the input corresponding to each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "225bf33b-e0f4-4d13-8cb8-fdac7703db3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5acf9c3b613541269cd8da27003e1405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6677b6c95ad5457ba64c0c579f0b6ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90bb79f4973f46acaa4e63c12d234ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea18d4e-00d3-40c4-9a85-0794d6633dee",
   "metadata": {},
   "source": [
    "#### Checking the length distribution of dialogue and summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdeb1b4c-8e06-4857-b8d6-f712209afb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (567 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "d_len = [len(tokenizer.encode(text)) for text in samsum['train']['dialogue']]\n",
    "s_len = [len(tokenizer.encode(text)) for text in samsum['train']['summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bc9e319-b7e5-4ce9-8f6a-ffa95957f264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAAD0CAYAAACGjNCJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAlwElEQVR4nO3dfbxdVX3n8c8XlCeRCiXSkIChms4ITI0lUqpOB0WHSLVgW5xYK9ixxlLaSmtVqHSqUzJDZ6y1TBWLSgG10vhUI4oVKdRxhgeDg0BASiwIkZQni4TqgIm/+WOvDJvLzc25N/fcx8/79Tqvs8/ae+39W+feu87vrrP23qkqJEmSJHV2me4AJEmSpJnEBFmSJEnqMUGWJEmSekyQJUmSpB4TZEmSJKnHBFmSJEnqMUHWUCV5f5I/GHDbK5P82rBjGrYkleRZ0x3HVJgrPzNJM1OSJa1PfdJ0xzIVktyR5CXTHYdMkLUT2h/y95NsTvJgkv+d5NeT/P/fq6r69ar6o+mMc6KSXJrk4fb4QZJHe6/fP82xvSPJR+b6MaW5KskLW5/53STfSfK/kjxvuuMapiTre33o1iT/t/f696c5tguSnDXXj6nBzYv/yDRUr6iqLyX5EeDfAX8G/DTwq9Mb1s6rqpdtW05yAbCxqs6cvogkzQVJ9gEuAU4B1gC7Af8WeGQ64xqvJAFSVT8cZPuqOqxX90rgI1X1wSGFJ+0UR5A1Karqu1W1FvgPwMlJDofH/4ecZN8klyS5L8k/t+XFo+0vyS5JzkzyrST3JrmoJeHb1p/U1j2Q5A/6X0uN/K88ydFJNvZeH5jkky2O25P89njbm+QNSTa0kZ+1SQ7cznYvTHJXkhe11/8xyS2t/X+b5Bm9bauNwN/W1r+3fQCNN7aj2sjUg0m+nuTo3rork/xRG63anOSLSfbvrR/1fU2yAvh94D+00Z6v9w75jO3tT9KofgKgqj5WVVur6vtV9cWqugGe+G1NRkwzaH/HZ7W/84eTfDbJjyb5aJKHknw1yZJe/UryG61v2dz6gGcmuaptvybJbm3bMfvpduzVSf4X8D3gzUmu6zcuyZuT/M2gb8aO+vsR2/5i65cOb/VOT/LN1metSbLfiPfs5CR3Jrk/ydsHjWnEMV+e5Po89k3pT/bW3ZHk95LckO7bgL9Oskdv/VuTbEpyd5JfazE9K8kq4DXAW7f9DHuHXLa9/WnqmCBrUlXVtcBGutGQkXYB/hJ4BnAw8H3gz7ezq9e1x4uAHwf23rZtkkOB99F1LguBHwEWDRJfuukfnwW+3uocA5yW5NhB6rd9vBj4r8Cr2vG/BVw8ynbHAh8DfrGqrkhyAl2S+QvAAuB/tvV9LweeBzyn7X/guNoxFwGfA84C9gN+D/hkkgW9zX6ZboT/6XQjV7/X6m73fa2qLwD/Bfjrqtq7qp6zo/1J2q5/ALYmuTDJy5LsO4F9rAReS/c3+kzgKrr+dT/gFuAPR2y/AjgCOAp4K3Ae3d/6QcDhwKvbdoP0068FVgFPBc4BDkny7N76XwE+PI62vI7t9Pd9SX4V+GPgJVV1E/DbwAl0314eCPwz8N4R1V4I/Cu6vv4/jYhzh5L8FHA+8EbgR4G/ANYm2b232avo3t9DgJ9sbaENLPwu8BLgWS1OAKrqPOCjwH9rfeordrQ/TS0TZA3D3XSd9ONU1QNV9cmq+l5VbQZW0+swRngN8O6q+seqehg4A1jZRlB+CfhsVX2lqh4F/hNQA8b2PGBBVf3nqnq0qv4R+ADdh82gXgOcX1Vfq6pHWmw/0x+xAU6k+wA6rv3TAF0H+1+r6paq2kKXcC5LbxQZOLuqHqyqO4ErgGXjiAu6D6bPV9Xnq+qHVXUZsA44rrfNX1bVP1TV9+m+3t12jIm+r9vbn6RRVNVDdIlb0fU/96X7JuqAcezmL6vqm1X1XeBS4JtV9aXWt3wceO6I7f+4qh6qqvXATcAXW/+6rf5zW2yD9NMXVNX6qtrS+sC/put7SHIYsIRuCsmgxurvtzkNeAtwdFVtaGVvBN5eVRtbHO8AfmlEvXe2Efqv0w2M9P+5H8QbgL+oqmvaaP+FdFNhjuptc05V3V1V36EbgFnWyl9F93NaX1XfA9454DG3tz9NIRNkDcMi4DsjC5PsleQv2tdoDwFfBp6WZNdR9nEg3cjsNt+imzN/QFt317YVreN5YMDYngEc2L4qezDJg3SjuuP5YHpcbK1Df4DHj2KfBqypqhtHHPvPesf9DpAR9f6pt/w9upGU8XgGcOKI9r2QbkR4R8eY6Pu6szFL8077R/l1VbWYbgT3QOA949jFPb3l74/yeuTf4UDbD9hP38XjXQj8cpLQjS6vaQnroMbq77d5C/DeqtrYK3sG8OleX3cLsHVEvcnoU988ok89qMW8o2M8rk/lie/b9tinzgAmyJpU6c7CXgR8ZZTVb6b7quunq2of4Ge3VRtl27vpOqZtDga20HXqm4D+nLg96b762uZfgL16r3+st3wXcHtVPa33eGpV9UdYd+RxsSV5Sjv+t3vbnAickOS0Ecd+44hj71lV/3scx96Ru4APjzjGU6rq7AHq7uh9HXSUXtI4VNU3gAvoEmUYuw8btkH66cf1BVV1NfAo3dS6X2Z80ytg7P5+m38PnJnkF3tldwEvG9Hf7VFV/b54Z90FrB5xjL2qauT0uNE8rk+lS6z77FNnMBNkTYok+yR5Od1c3I+MGDnd5ql0IxUPthMpRs6R6/sY8DtJDkmyN4/Nf90CfAJ4RZLnpzux5J08vvO+HjguyX5JfoxuNHeba4GHkrwtyZ5Jdm0ne4zn8kp/BfxqkmVtHtp/Aa6pqjt629xNN+ftt5P8Rit7P3BG+wqSJD+S5MRxHHekXZLs0XvsDnyE7r05trVtj3QnKY56MuQIO3pf7wGWpHcZP0njl+RfpzuRbXF7fRDdHOCr2ybXAz+b5OB0J6udMYXhjaef7ruIbt7wlqoabYBkLGP199usp5uX+94kP9/K3g+s3jZNLcmCJMeP89h9u47oU3ejmwLz60l+Op2nJPm5JE8dYH9r6D4rnp1kL7ppa3330M251gzkB5121meTbKb7L/vtwLvZ/iXe3gPsCdxP90HwhTH2ez7dKMSXgduB/wv8FkCbQ/dbdMn4JmAzcC+PXSLpw3Rzze4Avkg3P45WdyvwCro5Xbe3WD5Id0LaQKrqcuAPgE+24z+TUeYwt3nExwBvS/JrVfVpuhNMLm5fXd4EvGxkvXF4Nd0H2bbHN6vqLuB4umkj99H9XN7CAH/rA7yvH2/PDyT52k7ELc13m+kuh3lNkn+h6w9vohu9pZ078NfADcB1jG8+7856D4P3030fphsBH+/oMYzR3/e1ecQvBz6Q5GV0lxVdC3yxfQ5dTfe+TtTpPL5P/buqWkc3D/nP6U4C3MCAJ81V1aV0JzFe0epd1VZt61M/BBzapm78zU7ErSFIlSP8mt3aiMODwNKqun2aw5kzfF8lDapNyboX+Kmqum2645mJ2hU0bgJ2HzE6rhnIEWTNSkle0U4meQrwLuBGuhFj7QTfV0kTdArwVZPjx0vyyiS7pbuU3x/TXSnI5HgWMEHWbHU83Tzfu4GlwMry65DJ4PsqaVyS3AG8iTZFRI/zRrrpbt+ku8LGKdMbjgblFAtJkiSpxxFkSZIkqedJO95kdtp///1ryZIl0x2GJA3Fddddd39VLdjxlhNnPypprtteXzpnE+QlS5awbt266Q5DkoYiybd2vNXOsR+VNNdtry91ioUkSZLUY4IsSZIk9ZggS5IkST0myJIkSVKPCbIkSZLUY4IsSZIk9ZggS5IkST1z9jrIU235WZdx/8OPjrve/nvvxrozXzqEiCRJkjQRjiBPkokkxztTT5IkScNhgixJkiT1mCBLkiRJPUNPkJPsmuT/JLmkvd4vyWVJbmvP+/a2PSPJhiS3Jjm2V35EkhvbunOSZNhxS5IkaX6aihHkNwG39F6fDlxeVUuBy9trkhwKrAQOA1YA70uya6tzLrAKWNoeK6YgbkmSJM1DQ02QkywGfg74YK/4eODCtnwhcEKv/OKqeqSqbgc2AEcmWQjsU1VXVVUBF/XqSJIkSZNq2CPI7wHeCvywV3ZAVW0CaM9Pb+WLgLt6221sZYva8sjyJ0iyKsm6JOvuu+++SWmAJM0n9qOSNMQEOcnLgXur6rpBq4xSVmOUP7Gw6ryqWl5VyxcsWDDgYSVJ29iPStJwbxTyAuDnkxwH7AHsk+QjwD1JFlbVpjZ94t62/UbgoF79xcDdrXzxKOWSJEnSpBvaCHJVnVFVi6tqCd3Jd39XVb8CrAVObpudDHymLa8FVibZPckhdCfjXdumYWxOclS7esVJvTqSJEnSpJqOW02fDaxJ8nrgTuBEgKpan2QNcDOwBTi1qra2OqcAFwB7Ape2hyRJkjTppiRBrqorgSvb8gPAMdvZbjWwepTydcDhw4tQkiRJ6ngnPUmSJKnHBFmSJEnqMUGWJEmSekyQJUmSpB4TZEmSJKnHBFmSJEnqMUGWJEmSekyQJUmSpB4TZEmSJKnHBFmSJEnqMUGWJEmSekyQJUmSpJ6hJchJ9khybZKvJ1mf5J2t/B1Jvp3k+vY4rlfnjCQbktya5Nhe+RFJbmzrzkmSYcUtSZKk+e1JQ9z3I8CLq+rhJE8GvpLk0rbuT6vqXf2NkxwKrAQOAw4EvpTkJ6pqK3AusAq4Gvg8sAK4FEmSJGmSDW0EuToPt5dPbo8ao8rxwMVV9UhV3Q5sAI5MshDYp6quqqoCLgJOGFbckiRJmt+GOgc5ya5JrgfuBS6rqmvaqt9MckOS85Ps28oWAXf1qm9sZYva8sjy0Y63Ksm6JOvuu+++yWyKJM0L9qOSNOQEuaq2VtUyYDHdaPDhdNMlngksAzYBf9I2H21ecY1RPtrxzquq5VW1fMGCBTsZvSTNP/ajkjRFV7GoqgeBK4EVVXVPS5x/CHwAOLJtthE4qFdtMXB3K188SrkkSZI06YZ5FYsFSZ7WlvcEXgJ8o80p3uaVwE1teS2wMsnuSQ4BlgLXVtUmYHOSo9rVK04CPjOsuCVJkjS/DfMqFguBC5PsSpeIr6mqS5J8OMkyumkSdwBvBKiq9UnWADcDW4BT2xUsAE4BLgD2pLt6hVewkCRJ0lAMLUGuqhuA545S/tox6qwGVo9Svg44fFIDlCRJkkbhnfQkSZKkHhNkSZIkqccEWZIkSeoxQZYkSZJ6TJAlSZKkHhNkSZIkqccEWZIkSeoxQZYkSZJ6TJAlSZKkHhNkSZIkqccEWZIkSeoxQZYkSZJ6hpYgJ9kjybVJvp5kfZJ3tvL9klyW5Lb2vG+vzhlJNiS5NcmxvfIjktzY1p2TJMOKW5IkSfPbMEeQHwFeXFXPAZYBK5IcBZwOXF5VS4HL22uSHAqsBA4DVgDvS7Jr29e5wCpgaXusGGLckiRJmseGliBX5+H28sntUcDxwIWt/ELghLZ8PHBxVT1SVbcDG4AjkywE9qmqq6qqgIt6dSRJkqRJ9aRh7ryNAF8HPAt4b1Vdk+SAqtoEUFWbkjy9bb4IuLpXfWMr+0FbHlk+2vFW0Y00c/DBB08o5uVnXcb9Dz86obqSNNtNRj8qSbPdUE/Sq6qtVbUMWEw3Gnz4GJuPNq+4xigf7XjnVdXyqlq+YMGCcccLmBxLmtcmox+VpNluSq5iUVUPAlfSzR2+p02boD3f2zbbCBzUq7YYuLuVLx6lXJIkSZp0w7yKxYIkT2vLewIvAb4BrAVObpudDHymLa8FVibZPckhdCfjXdumY2xOclS7esVJvTqSJEnSpBrmHOSFwIVtHvIuwJqquiTJVcCaJK8H7gROBKiq9UnWADcDW4BTq2pr29cpwAXAnsCl7SFJkiRNuqElyFV1A/DcUcofAI7ZTp3VwOpRytcBY81fliRJkiaFd9KTJEmSekyQJUmSpJ6hXgdZg1ly+ucmVG//vXdj3ZkvneRoJEmS5jdHkGcxr9ksSZI0+UyQJUmSpB4TZEmSJKnHBFmSJEnqMUGWJEmSekyQJUmSpB4TZEmSJKnHBFmSJEnqMUGWJEmSeoaWICc5KMkVSW5Jsj7Jm1r5O5J8O8n17XFcr84ZSTYkuTXJsb3yI5Lc2NadkyTDiluSJEnz2zBvNb0FeHNVfS3JU4HrklzW1v1pVb2rv3GSQ4GVwGHAgcCXkvxEVW0FzgVWAVcDnwdWAJcOMXZJkiTNU0MbQa6qTVX1tba8GbgFWDRGleOBi6vqkaq6HdgAHJlkIbBPVV1VVQVcBJwwrLglSZI0v03JHOQkS4DnAte0ot9MckOS85Ps28oWAXf1qm1sZYva8sjy0Y6zKsm6JOvuu+++yWyCJM0L9qOSNGCCnOQFg5Rtp+7ewCeB06rqIbrpEs8ElgGbgD/Ztuko1WuM8icWVp1XVcuravmCBQsGCU+S1GM/KkmDjyD/jwHLHifJk+mS449W1acAquqeqtpaVT8EPgAc2TbfCBzUq74YuLuVLx6lXJIkSZp0Y56kl+RngOcDC5L8bm/VPsCuO6gb4EPALVX17l75wqra1F6+EripLa8F/irJu+lO0lsKXFtVW5NsTnIU3RSNkxggOZckaUeWn3UZ9z/86Ljr7b/3bqw786VDiEjSTLCjq1jsBuzdtntqr/wh4Jd2UPcFwGuBG5Nc38p+H3h1kmV00yTuAN4IUFXrk6wBbqa7Asap7QoWAKcAFwB70l29witYSJJ22kSS452pJ2l2GDNBrqq/B/4+yQVV9a3x7LiqvsLo84c/P0ad1cDqUcrXAYeP5/iSJEnSRAx6HeTdk5wHLOnXqaoXDyMoSZIkaboMmiB/HHg/8EFg6w62lSRJkmatQRPkLVV17lAjkSRJkmaAQS/z9tkkv5FkYZL9tj2GGpkkSZI0DQYdQT65Pb+lV1bAj09uOJIkSdL0GihBrqpDhh2IJEmSNBMMlCAnOWm08qq6aHLDkSRJkqbXoFMsntdb3gM4BvgaYIIsSZKkOWXQKRa/1X+d5EeADw8lIkmSJGkaDXoVi5G+ByydzEAkSZKkmWDQOcifpbtqBcCuwLOBNcMKSpIkSZoug85BfldveQvwraraOIR4JEmSpGk10BSLqvp74BvAU4F9gUd3VCfJQUmuSHJLkvVJ3tTK90tyWZLb2vO+vTpnJNmQ5NYkx/bKj0hyY1t3TpKMt6GSJEnSIAZKkJO8CrgWOBF4FXBNkl/aQbUtwJur6tnAUcCpSQ4FTgcur6qlwOXtNW3dSuAwYAXwviS7tn2dC6yim/e8tK2XJEmSJt2gUyzeDjyvqu4FSLIA+BLwie1VqKpNwKa2vDnJLcAi4Hjg6LbZhcCVwNta+cVV9Qhwe5INwJFJ7gD2qaqr2rEvAk4ALh20kZIkSdKgBk2Qd9mWHDcPMI4rYCRZAjwXuAY4oCXPVNWmJE9vmy0Cru5V29jKftCWR5aPdpxVdCPNHHzwwYOGN6stOf1z466z/967se7Mlw4hGkmz3XzsRyVppEGT3C8k+dskr0vyOuBzwOcHqZhkb+CTwGlV9dBYm45SVmOUP7Gw6ryqWl5VyxcsWDBIePPS/Q/vcAq5pHnKflSSdjCCnORZdCO+b0nyC8AL6RLWq4CP7mjnSZ5Mlxx/tKo+1YrvSbKwjR4vBLaNTG8EDupVXwzc3coXj1IuSZIkTbodjSC/B9gMUFWfqqrfrarfoRs9fs9YFduVJj4E3FJV7+6tWguc3JZPBj7TK1+ZZPckh9CdjHdtm46xOclRbZ8n9epIkiRJk2pHc5CXVNUNIwural2bVzyWFwCvBW5Mcn0r+33gbGBNktcDd9JdGYOqWp9kDXAz3RUwTq2qra3eKcAFwJ50J+d5gp4kSZKGYkcJ8h5jrNtzrIpV9RVGnz8McMx26qwGVo9Svg44fKzjSZIkSZNhRwnyV5O8oao+0C9so7/XDS8sSZLmpuVnXTahk6W9ApE0dXaUIJ8GfDrJa3gsIV4O7Aa8cohxSZI0J030SkJegUiaOmMmyFV1D/D8JC/isSkOn6uqvxt6ZJIkSdI0GOhGIVV1BXDFkGORJEmSpt3Ad8OTJEmS5oNBbzUtSdKMNdET33bGktM/N6XHkzR1HEGWJM16nsAmaTKZIEuSJEk9JsiSJElSjwmyJEmS1GOCLEmSJPWYIEuSJEk9Q0uQk5yf5N4kN/XK3pHk20mub4/jeuvOSLIhya1Jju2VH5HkxrbunCQZVsySJEnSMEeQLwBWjFL+p1W1rD0+D5DkUGAlcFir874ku7btzwVWAUvbY7R9SpIkSZNiaAlyVX0Z+M6Amx8PXFxVj1TV7cAG4MgkC4F9quqqqirgIuCEoQQsSZIkMT130vvNJCcB64A3V9U/A4uAq3vbbGxlP2jLI8tHlWQV3WgzBx988CSHLUlzn/3ozDaRu/ftv/durDvzpUOIRpq7pvokvXOBZwLLgE3An7Ty0eYV1xjlo6qq86pqeVUtX7BgwU6GKknzj/3o3ONdBqXxm9IEuaruqaqtVfVD4APAkW3VRuCg3qaLgbtb+eJRyiVJkqShmNIEuc0p3uaVwLYrXKwFVibZPckhdCfjXVtVm4DNSY5qV684CfjMVMYsSZKk+WVoc5CTfAw4Gtg/yUbgD4GjkyyjmyZxB/BGgKpan2QNcDOwBTi1qra2XZ1Cd0WMPYFL20OSJEkaiqElyFX16lGKPzTG9quB1aOUrwMOn8TQJEmSpO3yTnqSJElSjwmyJEmS1GOCLEmSJPWYIEuSJEk9JsiSJElSjwmyJEmS1GOCLEmSJPWYIEuSJEk9Q7tRiGa2Jad/btx19t97N9ad+dIhRCNJkjRzOIKsgd3/8KPTHYIkSdLQmSBLkiRJPSbIkiRJUs/QEuQk5ye5N8lNvbL9klyW5Lb2vG9v3RlJNiS5NcmxvfIjktzY1p2TJMOKWZIkSRrmSXoXAH8OXNQrOx24vKrOTnJ6e/22JIcCK4HDgAOBLyX5iaraCpwLrAKuBj4PrAAuHWLckqRpsvysyzzfQdK0G9oIclV9GfjOiOLjgQvb8oXACb3yi6vqkaq6HdgAHJlkIbBPVV1VVUWXbJ+AJGlOMjmWNBNM9WXeDqiqTQBVtSnJ01v5IroR4m02trIftOWR5aNKsoputJmDDz54EsOWpPnBfnRu8tKe0vjMlJP0RptXXGOUj6qqzquq5VW1fMGCBZMWnCTNF/aj2sbRfM1nUz2CfE+ShW30eCFwbyvfCBzU224xcHcrXzxKuSRJGrKJjDyDo8+a/aZ6BHktcHJbPhn4TK98ZZLdkxwCLAWubdMxNic5ql294qReHUmSNAM5+qzZbmgjyEk+BhwN7J9kI/CHwNnAmiSvB+4ETgSoqvVJ1gA3A1uAU9sVLABOobsixp50V6/wChaSJEkamqElyFX16u2sOmY7268GVo9Svg44fBJDkyRJkrZrppykJ0mSJM0IJsiSJElSjwmyJEmS1GOCLEmSJPWYIEuSJEk9JsiSJElSjwmyJEmS1DPVt5rWLDeR2456y1FJkjSbOIKsofOWo5IkaTYxQZYkSZJ6TJAlSZKkHucgS5KkSec5K5rNpmUEOckdSW5Mcn2Sda1svySXJbmtPe/b2/6MJBuS3Jrk2OmIWZIkDZfnrGimmM4pFi+qqmVVtby9Ph24vKqWApe31yQ5FFgJHAasAN6XZNfpCFiSJElz30yaYnE8cHRbvhC4EnhbK7+4qh4Bbk+yATgSuGoaYpQkSUPk1AzNBNM1glzAF5Ncl2RVKzugqjYBtOent/JFwF29uhtb2RMkWZVkXZJ1991335BCl6S5y35Us5FTMzTZpitBfkFV/RTwMuDUJD87xrYZpaxG27Cqzquq5VW1fMGCBZMRpyTNK/ajkjRNCXJV3d2e7wU+TTdl4p4kCwHa871t843AQb3qi4G7py5aSZIkzSdTPgc5yVOAXapqc1v+98B/BtYCJwNnt+fPtCprgb9K8m7gQGApcO1Uxy1JkmYu5y5rMk3HSXoHAJ9Osu34f1VVX0jyVWBNktcDdwInAlTV+iRrgJuBLcCpVbV1GuLWTphIxwV2XpKk4XHusrZnyhPkqvpH4DmjlD8AHLOdOquB1UMOTTOQnZckSZpq3mpakiRJ6jFBliRJknpMkCVJkqQeE2RJkiSpxwRZkiRJ6pmOy7xJ4+K1LSVJwzLRy5CG7dzWdwf8fJodHEHWnOTl4SRJwzSR5Bj8fJotTJAlSZKkHqdYSJIkTSGnDs58JsiSJEkz3P0PP2piPYVMkDVn2ZFIkuY75zxPjHOQpR47EkmSNGtGkJOsAP4M2BX4YFWdPc0haY5y5FnaOcvPusx/NiXNarMiQU6yK/Be4KXARuCrSdZW1c3TG5nUmejcMDC51txjcizNLH4+jd+sSJCBI4ENVfWPAEkuBo4HTJA16000ufYi9ZKkYZrPn0+zJUFeBNzVe70R+OmRGyVZBaxqLx9OcusEjrU/cP8E6s0kc6ENMDfaMePa8C0gfzDuajOuHRMwF9oAj7XjGcPY+Tj70bnyno6X7Z5fbPcUmeDn084atS+dLQlyRil7wj8nVXUecN5OHShZV1XLd2Yf020utAHmRjvmQhtgbrRjLrQBht+O8fSjc+U9HS/bPb/Y7vlptlzFYiNwUO/1YuDuaYpFkiRJc9hsSZC/CixNckiS3YCVwNppjkmSJElz0KyYYlFVW5L8JvC3dJd5O7+q1g/pcDs1RWOGmAttgLnRjrnQBpgb7ZgLbYCZ1Y6ZFMtUst3zi+2eh1I1kfMMJUmSpLlptkyxkCRJkqaECbIkSZLUY4LcJFmR5NYkG5KcPt3xbE+Sg5JckeSWJOuTvKmV75fksiS3ted9e3XOaO26Ncmx0xf9EyXZNcn/SXJJez2r2pHkaUk+keQb7WfyM7OtDQBJfqf9Pt2U5GNJ9pjp7UhyfpJ7k9zUKxt3zEmOSHJjW3dOktEuKznV7fjv7XfqhiSfTvK0mdaO2dJn7oyJ9LdzyXj657livH36XDHez4D5wASZx93K+mXAocCrkxw6vVFt1xbgzVX1bOAo4NQW6+nA5VW1FLi8vaatWwkcBqwA3tfaO1O8Cbil93q2tePPgC9U1b8GnkPXllnVhiSLgN8GllfV4XQnwq5k5rfjgnb8vonEfC7djTGWtsfIfQ7bBaMc8zLg8Kr6SeAfgDNg5rRjlvWZO2Nc/e0cNFD/PMcM3KfPFeP9DJgvTJA7//9W1lX1KLDtVtYzTlVtqqqvteXNdH+8i+jivbBtdiFwQls+Hri4qh6pqtuBDXTtnXZJFgM/B3ywVzxr2pFkH+BngQ8BVNWjVfUgs6gNPU8C9kzyJGAvuuuMz+h2VNWXge+MKB5XzEkWAvtU1VXVnbF8Ua/OlBitHVX1xara0l5eTXftd5g57Zg1febOmEB/O2eMs3+eEybQp88l4/kMmBdMkDuj3cp60TTFMrAkS4DnAtcAB1TVJug6deDpbbOZ3Lb3AG8Fftgrm03t+HHgPuAv29eQH0zyFGZXG6iqbwPvAu4ENgHfraovMsva0Yw35kVteWT5TPIfgUvb8kxpx0z+HRiKAfvbueQ9DN4/zxXj7dPnhAl8BswLJsidgW5lPZMk2Rv4JHBaVT001qajlE1725K8HLi3qq4btMooZdPdjicBPwWcW1XPBf6Fsb+CmoltoM0rOx44BDgQeEqSXxmryihl096OHdhezDO6LUneTvc1/0e3FY2y2XS0Y0a/b5NtHP3tnDCB/nmuGG+fPidM4DNgXjBB7syqW1kneTJdZ/3RqvpUK76nfc1Ke763lc/Utr0A+Pkkd9B9PfviJB9hdrVjI7Cxqq5prz9B17nOpjYAvAS4varuq6ofAJ8Cns/saweMP+aNPDZ9oV8+7ZKcDLwceE09dsH6mdKOmfw7MKnG2d/OFePtn+eK8fbpc8V4PwPmBRPkzqy5lXU7M/1DwC1V9e7eqrXAyW35ZOAzvfKVSXZPcgjdyTvXTlW821NVZ1TV4qpaQvd+/11V/QqzqB1V9U/AXUn+VSs6BriZWdSG5k7gqCR7td+vY+jmWs62dsA4Y25fG25OclRr+0m9OtMmyQrgbcDPV9X3eqtmSjtmTZ+5MybQ384JE+if54QJ9OlzxXg/A+aHqvLRDc4cR3e2+DeBt093PGPE+UK6rzJvAK5vj+OAH6U7y/S29rxfr87bW7tuBV423W0YpU1HA5e05VnVDmAZsK79PP4G2He2taHF9U7gG8BNwIeB3Wd6O4CP0c2X+wHdyM/rJxIzsLy1+5vAn9PuMDrN7dhAN8d329/4+2daO2ZLn7mTbRx3fzvXHoP2z3PlMd4+fa48xvsZMB8e3mpakiRJ6nGKhSRJktRjgixJkiT1mCBLkiRJPSbIkiRJUo8JsiRJktRjgiwBSR4e8v5PS7LXVB1PkqaDfanmChNkaWqcBuy1o40kSWM6DftSTYEnTXcA0kyV5JnAe4EFwPeAN1TVN5JcADxEd2OGHwPeWlWfSLIL3Q0a/h1wO90/oOfT3dv+QOCKJPdX1Yva/lfT3Ur4+8DxVXXPVLZPkqaCfalmI0eQpe07D/itqjoC+D3gfb11C+nusvVy4OxW9gvAEuDfAL8G/AxAVZ0D3A28aFuHDjwFuLqqngN8GXjDUFsiSdPHvlSzjiPI0iiS7A08H/h4d2t6oLv15jZ/U1U/BG5OckAreyHw8Vb+T0muGOMQjwKXtOXrgJdOWvCSNEPYl2q2MkGWRrcL8GBVLdvO+kd6yxnxPIgf1GP3ed+Kf4uS5ib7Us1KTrGQRlFVDwG3JzkRIJ3n7KDaV4BfTLJLGwk5urduM/DUoQQrSTOUfalmKxNkqbNXko29x+8CrwFen+TrwHrg+B3s45PARuAm4C+Aa4DvtnXnAZfu4KtCSZrt7Es1J+SxbyYk7awke1fVw0l+FLgWeEFV/dN0xyVJs4l9qaabc3WkyXVJkqcBuwF/ZIcuSRNiX6pp5QiyJEmS1OMcZEmSJKnHBFmSJEnqMUGWJEmSekyQJUmSpB4TZEmSJKnn/wFc24BnGvXIxAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x252 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True)\n",
    "\n",
    "axes[0].hist(d_len, bins=20, color=\"C0\", edgecolor=\"C0\")\n",
    "axes[0].set_title(\"Dialogue Token Length\")\n",
    "axes[0].set_xlabel(\"Length\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[1].hist(s_len, bins=20, color=\"C0\", edgecolor=\"C0\")\n",
    "axes[1].set_title(\"Summary Token Length\")\n",
    "axes[1].set_xlabel(\"Length\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde6efd3-3bf1-4fce-8c46-c1b49c8a349a",
   "metadata": {},
   "source": [
    "- Most of our dialogue lengths are between 100-200 tokens per dialogue and summary lenth is between 20-40 tokens per dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abb24c12-fa0d-4873-94d9-b793483f142d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 64% | 10% |\n"
     ]
    }
   ],
   "source": [
    "from GPUtil import showUtilization as gpu_usage\n",
    "gpu_usage()             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810294de-3ead-45d9-ba76-02c6f5ac93ee",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e6a49e-4958-4772-94d7-876bd409686c",
   "metadata": {},
   "source": [
    "### Let us tokenize the dataset\n",
    "- We’ll set the maximum lengths to 1024 for the dialogues and 128 for the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a81e739-0772-4ac5-851b-37e4e1f7be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "    \n",
    "    input_encodings = tokenizer(example_batch[\"dialogue\"],\n",
    "                                max_length=1024,\n",
    "                                truncation=True)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(example_batch[\"summary\"],\n",
    "                                     max_length=128,\n",
    "                                     truncation=True)\n",
    "        \n",
    "    return {\"input_ids\": input_encodings[\"input_ids\"],\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "            \"labels\": target_encodings[\"input_ids\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0449150c-6c56-4b6e-b47d-78b4ddb60f9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a71e3556944aa68928a9f312195835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc9c61c28a541e99aa202ceb9ddb3ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e3115d674b4f0fa9812b79e416c035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_samsum_pt = samsum.map(convert_examples_to_features, \n",
    "                               batched=True)\n",
    "\n",
    "columns = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
    "dataset_samsum_pt.set_format(type=\"torch\", columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e5decd-1706-4d0c-95f6-8b2667dff8be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f87ce427-4f91-4b9f-a72f-c807d7b8a6f7",
   "metadata": {},
   "source": [
    "### Create Data Collator\n",
    "- This function is called in the Trainer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55781600-23f4-41e7-bd16-4ad0f4c8a1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model = model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f91bed1-e23f-41a8-ab6a-f4bac0bc5b35",
   "metadata": {},
   "source": [
    "### Setting up TrainingArguments for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8bb7881-fd42-4476-a0d3-0ac84e868d8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You need to pass a valid `token` or login by using `huggingface-cli login`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a47148a21f4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m      4\u001b[0m                                   \u001b[0;34m't5-small-samsum'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                   \u001b[0mnum_train_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-gpu/lib/python3.9/site-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_on_each_node, no_cuda, seed, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, xpu_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, deepspeed, label_smoothing_factor, optim, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, gradient_checkpointing, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-gpu/lib/python3.9/site-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub_model_id\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m             self.hub_model_id = get_full_repo_name(\n\u001b[0m\u001b[1;32m    937\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub_model_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morganization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub_organization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-gpu/lib/python3.9/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_full_repo_name\u001b[0;34m(model_id, organization, token)\u001b[0m\n\u001b[1;32m   2931\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHfFolder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2932\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0morganization\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2933\u001b[0;31m         \u001b[0musername\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhoami\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2934\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34mf\"{username}/{model_id}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2935\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-gpu/lib/python3.9/site-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mwhoami\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHfFolder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    450\u001b[0m                 \u001b[0;34m\"You need to pass a valid `token` or login by using `huggingface-cli login`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: You need to pass a valid `token` or login by using `huggingface-cli login`"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments('t5-small-samsum',\n",
    "                                  num_train_epochs=1, \n",
    "                                  warmup_steps=500,\n",
    "                                  per_device_train_batch_size=1, \n",
    "                                  per_device_eval_batch_size=1,\n",
    "                                  weight_decay=0.01,\n",
    "                                  logging_steps=10, \n",
    "                                  push_to_hub=True,\n",
    "                                  push_to_hub_model_id='t5-small-samsum',  \n",
    "                                  evaluation_strategy='steps',\n",
    "                                  eval_steps=500,\n",
    "                                  save_steps=1e6,\n",
    "                                  gradient_accumulation_steps=16\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd92f9c3-8227-4596-a9a0-3396b3e8bfec",
   "metadata": {},
   "source": [
    "- Logging into HuggingFace so that we can push our model to the Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8baf6f6-1f23-42e6-a8ae-13494d9eefa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66302c28-c94c-4a4d-b2e5-d46cd185140a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(model = model, \n",
    "                  args = training_args,\n",
    "                  tokenizer = tokenizer,\n",
    "                  data_collator = seq2seq_data_collator,\n",
    "                  train_dataset = dataset_samsum_pt[\"train\"],\n",
    "                  eval_dataset = dataset_samsum_pt[\"validation\"]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8b769e-161e-4c1a-8e71-43cd09bbb6b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c9ec84-0139-49e8-9fe6-72c5aa671584",
   "metadata": {},
   "source": [
    "- Pusing our model to the hub so that we can easily use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d444d187-7094-49d4-b2c2-dc3a4e571d8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub(\"Training finally completed!!!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ce55f6-ae77-4ea6-9aa2-d3714b1208a8",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0543226c-64ee-4402-923b-fb1e60f3e194",
   "metadata": {},
   "source": [
    "## Testing our model on random conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a5b8251-9923-40ae-8459-360425c2143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = '''\n",
    "Aakash: Do you watched last night match?\n",
    "Vikas: No, I was busy. What happended?\n",
    "Aakash: Barcelona won their game against Sevilla 1-0. Pedri scored an sensational goal.\n",
    "Vikas: Oh really!!! I have to watch the highlights at home.\n",
    "Aakash: Yes, I am so excited for the new upcoming season. Xavi really doing wonders.\n",
    "Vikas: Exactly!! Xavi has changed the way Barcelona play. The good days are coming back. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52e7dc39-457b-40ec-801a-ee54109bc54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d63769ab-9de0-4d3e-8438-ad34d6e7528d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "No such file or directory (os error 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2adad13526c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummarizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'summarization'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'anegi/t5-small-samsum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch-gpu/lib/python3.9/site-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m                 \u001b[0mtokenizer_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m             tokenizer = AutoTokenizer.from_pretrained(\n\u001b[0m\u001b[1;32m    599\u001b[0m                 \u001b[0mtokenizer_identifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_fast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_from_pipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtokenizer_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-gpu/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m                     \u001b[0;34mf\"Tokenizer class {tokenizer_class_candidate} does not exist or is not currently imported.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m                 )\n\u001b[0;32m--> 528\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;31m# Otherwise we have to be creative.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-gpu/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1780\u001b[0;31m         return cls._from_pretrained(\n\u001b[0m\u001b[1;32m   1781\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-gpu/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;31m# Instantiate tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1915\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1916\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             raise OSError(\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-gpu/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, tokenizer_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 )\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mtokenizer_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-gpu/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mfast_tokenizer_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfrom_slow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;31m# We have a serialization from tokenizers which let us directly build the backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mfast_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizerFast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfast_tokenizer_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mslow_tokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;31m# We need to convert a slow tokenizer to build the backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: No such file or directory (os error 2)"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline('summarization', model='anegi/t5-small-samsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e65c2584-7e52-4553-95d5-3f68d675d6e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Pedri scored a sensational goal for Barcelona against Sevilla . Xavi has changed the way they play. Aakash is excited for the new season.'}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(conversation, max_length=90, min_length = 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f628b481-d620-486e-8c97-c87944d9a276",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d805be7-04d4-4069-aceb-323e8858eb32",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Rogue - Score\n",
    "#### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aa4ca505-015a-48df-8469-82741264296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dcc45d35-9c99-486c-9784-d5c115c60f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL','rougeLsum'],\n",
    "                                  use_stemmer=True)\n",
    "# Adding a summary that a human would generate when the above conversation is provided to them.\n",
    "\n",
    "scores = scorer.score(conversation,\n",
    "                      'Aakash asked Vikas about the match which Barcelona won where Pedri scored the only goal.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e7e3dc8c-7ca9-4137-b180-f4482eee6efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.6666666666666666, recall=0.14492753623188406, fmeasure=0.2380952380952381),\n",
       " 'rouge2': Score(precision=0.14285714285714285, recall=0.029411764705882353, fmeasure=0.04878048780487805),\n",
       " 'rougeL': Score(precision=0.4666666666666667, recall=0.10144927536231885, fmeasure=0.16666666666666669),\n",
       " 'rougeLsum': Score(precision=0.6666666666666666, recall=0.14492753623188406, fmeasure=0.2380952380952381)}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0cdb96-758c-4dd7-b3c2-1daef273f5b0",
   "metadata": {},
   "source": [
    "- Converting the scores to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3fc04ae6-6403-449d-86cb-54108c3063ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score = pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f4184af6-96d7-4547-82c0-13ea2fc6cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score.index = ['precision','recall','fmeasure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9bf9969e-dd3d-4d06-ac0f-1ac16500fcb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.144928</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.101449</td>\n",
       "      <td>0.144928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fmeasure</th>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.238095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             rouge1    rouge2    rougeL  rougeLsum\n",
       "precision  0.666667  0.142857  0.466667   0.666667\n",
       "recall     0.144928  0.029412  0.101449   0.144928\n",
       "fmeasure   0.238095  0.048780  0.166667   0.238095"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebfc4c2-238c-47a3-8677-9d737ce8bb16",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0535f1e-63e0-4644-afc7-307ee231e630",
   "metadata": {},
   "source": [
    "#### Method 2\n",
    "\n",
    "- The ROUGE metric in the HuggingFace Datasets library also calculates confidence intervals\n",
    "  ( by default, the 5th and 95th percentiles). The average value is stored in the attribute mid and\n",
    "   the interval can be retrieved with low and high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "238f8fce-7060-4213-831e-2ef71c5ebfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "rougue_metric = load_metric('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e857366e-6e4e-498d-b148-d3f2a3379393",
   "metadata": {},
   "outputs": [],
   "source": [
    "rougue_metric.add(prediction = 'Aakash asked Vikas about the match which Barcelona won where Pedri scored the only goal.',\n",
    "                  reference = [conversation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4ca4f5d2-a04a-4bf6-a80c-b03d1b323d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "score2 = rougue_metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "eb0ea1f8-33fb-4918-9d96-44c91c49b201",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=0.5333333333333333, recall=0.11428571428571428, fmeasure=0.18823529411764706), mid=Score(precision=0.5333333333333333, recall=0.11428571428571428, fmeasure=0.18823529411764706), high=Score(precision=0.5333333333333333, recall=0.11428571428571428, fmeasure=0.18823529411764706)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=0.14285714285714285, recall=0.028985507246376812, fmeasure=0.048192771084337345), mid=Score(precision=0.14285714285714285, recall=0.028985507246376812, fmeasure=0.048192771084337345), high=Score(precision=0.14285714285714285, recall=0.028985507246376812, fmeasure=0.048192771084337345)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=0.4, recall=0.08571428571428572, fmeasure=0.1411764705882353), mid=Score(precision=0.4, recall=0.08571428571428572, fmeasure=0.1411764705882353), high=Score(precision=0.4, recall=0.08571428571428572, fmeasure=0.1411764705882353)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=0.4, recall=0.08571428571428572, fmeasure=0.1411764705882353), mid=Score(precision=0.4, recall=0.08571428571428572, fmeasure=0.1411764705882353), high=Score(precision=0.4, recall=0.08571428571428572, fmeasure=0.1411764705882353))}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a27c26e-989d-422a-8209-bfe0d628aefd",
   "metadata": {},
   "source": [
    "- Converting the scores into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8ef60b94-2c78-430c-8ef6-5bb353ad1f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(score2, orient=\"index\").stack().to_frame()\n",
    "\n",
    "# to break out the lists into columns\n",
    "\n",
    "df_score2 = pd.DataFrame(df[0].values.tolist(), index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3ae21bbc-6632-4715-befb-c2ccb85ad029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">rouge1</th>\n",
       "      <th>low</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.188235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mid</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.188235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.188235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">rouge2</th>\n",
       "      <th>low</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.048193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mid</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.048193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.048193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">rougeL</th>\n",
       "      <th>low</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.141176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mid</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.141176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.141176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">rougeLsum</th>\n",
       "      <th>low</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.141176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mid</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.141176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.141176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                precision    recall  fmeasure\n",
       "rouge1    low    0.533333  0.114286  0.188235\n",
       "          mid    0.533333  0.114286  0.188235\n",
       "          high   0.533333  0.114286  0.188235\n",
       "rouge2    low    0.142857  0.028986  0.048193\n",
       "          mid    0.142857  0.028986  0.048193\n",
       "          high   0.142857  0.028986  0.048193\n",
       "rougeL    low    0.400000  0.085714  0.141176\n",
       "          mid    0.400000  0.085714  0.141176\n",
       "          high   0.400000  0.085714  0.141176\n",
       "rougeLsum low    0.400000  0.085714  0.141176\n",
       "          mid    0.400000  0.085714  0.141176\n",
       "          high   0.400000  0.085714  0.141176"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7a353b-501d-4f30-ac16-425b657392d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f2d4bb9-ec5f-467c-91aa-0359a2d3b120",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
